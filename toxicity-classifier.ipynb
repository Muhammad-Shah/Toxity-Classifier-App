{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-03T13:15:28.528920Z","iopub.status.busy":"2024-05-03T13:15:28.528517Z","iopub.status.idle":"2024-05-03T13:15:29.551747Z","shell.execute_reply":"2024-05-03T13:15:29.550718Z","shell.execute_reply.started":"2024-05-03T13:15:28.528888Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n","/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\n","/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n","/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:01:13.683499Z","iopub.status.busy":"2024-04-30T20:01:13.682872Z","iopub.status.idle":"2024-04-30T20:01:13.688724Z","shell.execute_reply":"2024-04-30T20:01:13.687536Z","shell.execute_reply.started":"2024-04-30T20:01:13.683428Z"},"trusted":true},"outputs":[],"source":["# /kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip"]},{"cell_type":"code","execution_count":3,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-04-30T20:01:13.887434Z","iopub.status.busy":"2024-04-30T20:01:13.887001Z","iopub.status.idle":"2024-04-30T20:01:16.924141Z","shell.execute_reply":"2024-04-30T20:01:16.922293Z","shell.execute_reply.started":"2024-04-30T20:01:13.887401Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: not a git repository (or any parent up to mount point /kaggle)\n","Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"]}],"source":["# !git clone https://github.com/Muhammad-Shah/Toxity-Classifier-App.git\n","# !git status\n","# !git add --all\n","!git config --global user.email \"\"\n","!git config --global user.name \"\"\n","# !git commit -m \"testing\"\n","!git push"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:01:16.928555Z","iopub.status.busy":"2024-04-30T20:01:16.927372Z","iopub.status.idle":"2024-04-30T20:01:16.938690Z","shell.execute_reply":"2024-04-30T20:01:16.937300Z","shell.execute_reply.started":"2024-04-30T20:01:16.928461Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/kaggle/working/Toxity-Classifier-App'\n","/kaggle/working\n"]}],"source":["%cd /kaggle/working/Toxity-Classifier-App"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T13:15:35.860225Z","iopub.status.busy":"2024-05-03T13:15:35.859128Z","iopub.status.idle":"2024-05-03T13:15:45.803558Z","shell.execute_reply":"2024-05-03T13:15:45.802068Z","shell.execute_reply.started":"2024-05-03T13:15:35.860182Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","[nltk_data] Downloading package wordnet2022 to /usr/share/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet2022.zip.\n"]}],"source":["import re\n","import pandas as pd\n","import numpy as np\n","import spacy\n","import random\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.preprocessing import LabelEncoder\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import nltk\n","from spacy import load\n","nltk.download('omw-1.4')\n","nltk.download('wordnet2022')\n","nlp = load('en_core_web_sm')\n","\n","! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet # temp fix for lookup error.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Toxity Classifier NLP Project\n","====================================="]},{"cell_type":"markdown","metadata":{},"source":["## 1. Introduction\n","---------------"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1 Project Overview\n","\n","* This project aims to develop a toxicity classifier to identify toxic comments online.\n","* The proliferation of toxic comments online has significant negative impacts on individuals and communities, necessitating the development of effective detection and mitigation strategies."]},{"cell_type":"markdown","metadata":{},"source":["### 1.2 Problem Statement\n","\n","* Classify text as toxic or non-toxic to prevent online harassment and promote respectful communication.\n","* To create a safer and more inclusive online environment, free from hate speech and cyberbullying."]},{"cell_type":"markdown","metadata":{},"source":["## 2. Data Exploration\n","---------------------"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1 Data Loading"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.1.1 Load Dataset\n","\n","* Load the dataset into a Pandas dataframe\n","* Display the first few rows of the data using `head()`"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:49:54.131565Z","iopub.status.busy":"2024-04-30T20:49:54.131175Z","iopub.status.idle":"2024-04-30T20:49:56.068374Z","shell.execute_reply":"2024-04-30T20:49:56.067520Z","shell.execute_reply.started":"2024-04-30T20:49:54.131538Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\nWhy the edits made under my usern...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id                                       comment_text  toxic  \\\n","0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n","1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n","2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n","3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n","4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n","\n","   severe_toxic  obscene  threat  insult  identity_hate  \n","0             0        0       0       0              0  \n","1             0        0       0       0              0  \n","2             0        0       0       0              0  \n","3             0        0       0       0              0  \n","4             0        0       0       0              0  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["train_data = pd.read_csv(filepath_or_buffer='/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\n","train_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.1.2 Data Source\n","\n","* **Data Source:** Jigsaw Toxic Comment Classification Challenge on Kaggle\n","* **Data Collection:** The dataset was collected from Wikipedia comments and annotated by human raters for toxicity.\n","* **Data Description:** The dataset consists of a CSV file containing approximately 159,571 comments, each labeled as toxic or non-toxic.\n","* **Data Download:** The dataset can be downloaded from the Kaggle competition page: [/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip](/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1.3 Dataset Description\n","\n","You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n","\n","* toxic\n","* severe_toxic\n","* obscene\n","* threat\n","* insult\n","* identity_hate\n","\n","`You must create a model which predicts a probability of each type of toxicity for each comment`"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2 Data Summary"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.1 Data Statistics\n","\n","* Provide a summary of the data using `info()` and `describe()`\n","* Visualize the data distribution using plots (e.g., histograms, box plots)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:49:56.070120Z","iopub.status.busy":"2024-04-30T20:49:56.069793Z","iopub.status.idle":"2024-04-30T20:49:56.133424Z","shell.execute_reply":"2024-04-30T20:49:56.132493Z","shell.execute_reply.started":"2024-04-30T20:49:56.070095Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 159571 entries, 0 to 159570\n","Data columns (total 8 columns):\n"," #   Column         Non-Null Count   Dtype \n","---  ------         --------------   ----- \n"," 0   id             159571 non-null  object\n"," 1   comment_text   159571 non-null  object\n"," 2   toxic          159571 non-null  int64 \n"," 3   severe_toxic   159571 non-null  int64 \n"," 4   obscene        159571 non-null  int64 \n"," 5   threat         159571 non-null  int64 \n"," 6   insult         159571 non-null  int64 \n"," 7   identity_hate  159571 non-null  int64 \n","dtypes: int64(6), object(2)\n","memory usage: 9.7+ MB\n"]}],"source":["train_data.info(verbose=True)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:49:56.134682Z","iopub.status.busy":"2024-04-30T20:49:56.134405Z","iopub.status.idle":"2024-04-30T20:49:56.180734Z","shell.execute_reply":"2024-04-30T20:49:56.179409Z","shell.execute_reply.started":"2024-04-30T20:49:56.134660Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>159571.000000</td>\n","      <td>159571.000000</td>\n","      <td>159571.000000</td>\n","      <td>159571.000000</td>\n","      <td>159571.000000</td>\n","      <td>159571.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.095844</td>\n","      <td>0.009996</td>\n","      <td>0.052948</td>\n","      <td>0.002996</td>\n","      <td>0.049364</td>\n","      <td>0.008805</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.294379</td>\n","      <td>0.099477</td>\n","      <td>0.223931</td>\n","      <td>0.054650</td>\n","      <td>0.216627</td>\n","      <td>0.093420</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               toxic   severe_toxic        obscene         threat  \\\n","count  159571.000000  159571.000000  159571.000000  159571.000000   \n","mean        0.095844       0.009996       0.052948       0.002996   \n","std         0.294379       0.099477       0.223931       0.054650   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         0.000000       0.000000       0.000000       0.000000   \n","50%         0.000000       0.000000       0.000000       0.000000   \n","75%         0.000000       0.000000       0.000000       0.000000   \n","max         1.000000       1.000000       1.000000       1.000000   \n","\n","              insult  identity_hate  \n","count  159571.000000  159571.000000  \n","mean        0.049364       0.008805  \n","std         0.216627       0.093420  \n","min         0.000000       0.000000  \n","25%         0.000000       0.000000  \n","50%         0.000000       0.000000  \n","75%         0.000000       0.000000  \n","max         1.000000       1.000000  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["train_data.describe()"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.2 Data Quality\n","\n","* Check for missing values and handle them accordingly\n","* Check for duplicates and handle them accordingly"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:49:56.183634Z","iopub.status.busy":"2024-04-30T20:49:56.183192Z","iopub.status.idle":"2024-04-30T20:49:56.234946Z","shell.execute_reply":"2024-04-30T20:49:56.233729Z","shell.execute_reply.started":"2024-04-30T20:49:56.183594Z"},"trusted":true},"outputs":[{"data":{"text/plain":["id               0\n","comment_text     0\n","toxic            0\n","severe_toxic     0\n","obscene          0\n","threat           0\n","insult           0\n","identity_hate    0\n","dtype: int64"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train_data.isna().sum()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:49:56.237332Z","iopub.status.busy":"2024-04-30T20:49:56.236891Z","iopub.status.idle":"2024-04-30T20:49:56.483587Z","shell.execute_reply":"2024-04-30T20:49:56.482457Z","shell.execute_reply.started":"2024-04-30T20:49:56.237294Z"},"trusted":true},"outputs":[{"data":{"text/plain":["False    159571\n","Name: count, dtype: int64"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["columns = list(train_data.columns[1:])\n","train_data.duplicated(subset=columns).value_counts()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:49:56.485527Z","iopub.status.busy":"2024-04-30T20:49:56.485173Z","iopub.status.idle":"2024-04-30T20:49:56.579300Z","shell.execute_reply":"2024-04-30T20:49:56.578306Z","shell.execute_reply.started":"2024-04-30T20:49:56.485500Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of duplicates: 0\n","Empty DataFrame\n","Columns: [id, comment_text, toxic, severe_toxic, obscene, threat, insult, identity_hate]\n","Index: []\n"]}],"source":["duplicates = train_data[train_data.duplicated(subset='comment_text', keep=False)]\n","print(f\"Number of duplicates: {len(duplicates)}\")\n","print(duplicates)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:49:56.689114Z","iopub.status.busy":"2024-04-30T20:49:56.688773Z","iopub.status.idle":"2024-04-30T20:49:56.704110Z","shell.execute_reply":"2024-04-30T20:49:56.703272Z","shell.execute_reply.started":"2024-04-30T20:49:56.689088Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique values in column toxic: [0 1]\n","Unique values in column severe_toxic: [0 1]\n","Unique values in column obscene: [0 1]\n","Unique values in column threat: [0 1]\n","Unique values in column insult: [0 1]\n","Unique values in column identity_hate: [0 1]\n"]}],"source":["for col in train_data.columns[2:]:\n","    print(f\"Unique values in column {col}: {train_data[col].unique()}\")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:49:56.915267Z","iopub.status.busy":"2024-04-30T20:49:56.914561Z","iopub.status.idle":"2024-04-30T20:49:56.935288Z","shell.execute_reply":"2024-04-30T20:49:56.934199Z","shell.execute_reply.started":"2024-04-30T20:49:56.915224Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The distribution of toxicity labels in column: toxic\n","0    144277\n","1     15294\n","Name: count, dtype: int64\n","The distribution of toxicity labels in column: severe_toxic\n","0    157976\n","1      1595\n","Name: count, dtype: int64\n","The distribution of toxicity labels in column: obscene\n","0    151122\n","1      8449\n","Name: count, dtype: int64\n","The distribution of toxicity labels in column: threat\n","0    159093\n","1       478\n","Name: count, dtype: int64\n","The distribution of toxicity labels in column: insult\n","0    151694\n","1      7877\n","Name: count, dtype: int64\n","The distribution of toxicity labels in column: identity_hate\n","0    158166\n","1      1405\n","Name: count, dtype: int64\n"]}],"source":["for col in train_data.columns[2:]:\n","    print(f\"The distribution of toxicity labels in column: {train_data[col].value_counts()}\")"]},{"cell_type":"markdown","metadata":{},"source":["### 2.3 Data Quality Check"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.3.1 Handle Missing Values\n","\n","* Explain how missing values were handled\n","* Provide justification for the chosen method"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:49:58.540900Z","iopub.status.busy":"2024-04-30T20:49:58.540538Z","iopub.status.idle":"2024-04-30T20:49:58.545296Z","shell.execute_reply":"2024-04-30T20:49:58.544294Z","shell.execute_reply.started":"2024-04-30T20:49:58.540874Z"},"trusted":true},"outputs":[],"source":["# NO Missing values are found"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.3.2 Handle Duplicates\n","\n","* Explain how duplicates were handled\n","* Provide justification for the chosen method"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:49:59.492830Z","iopub.status.busy":"2024-04-30T20:49:59.492426Z","iopub.status.idle":"2024-04-30T20:49:59.498586Z","shell.execute_reply":"2024-04-30T20:49:59.497310Z","shell.execute_reply.started":"2024-04-30T20:49:59.492800Z"},"trusted":true},"outputs":[],"source":["# NO Duplicates are found"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Data Preprocessing\n","----------------------"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1 Text Preprocessing\n","\n","Text preprocessing is the process of cleaning and transforming raw text data into a format that is suitable for analysis or modeling by removing unnecessary characters, converting all text to lowercase, removing stop words, and stemming or lemmatizing words to their base form."]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:50:01.364674Z","iopub.status.busy":"2024-04-30T20:50:01.364266Z","iopub.status.idle":"2024-04-30T20:50:01.377376Z","shell.execute_reply":"2024-04-30T20:50:01.376202Z","shell.execute_reply.started":"2024-04-30T20:50:01.364636Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Geez, are you forgetful!  We've already discussed why Marx  was  not an anarchist, i.e. he wanted to use a State to mold his 'socialist man.'  Ergo, he is a statist - the opposite of an  anarchist.  I know a guy who says that, when he gets old and his teeth fall out, he'll quit eating meat.  Would you call him a vegetarian?\n","------------------------------------------------------------\n","Carioca RFA \n","\n","Thanks for your support on my request for adminship.\n","\n","The final outcome was (31/4/1), so I am now an administrator. If you have any comments or concerns on my actions as an administrator, please let me know. Thank you!\n","------------------------------------------------------------\n","\"\n","\n"," Birthday \n","\n","No worries, It's what I do ;)Enjoy ur day|talk|e \"\n","------------------------------------------------------------\n","Pseudoscience category? \n","\n","I'm assuming that this article is in the pseudoscience category because of its association with creationism.  However, there are modern, scientifically-accepted variants of catastrophism that have nothing to do with creationism — and they're even mentioned in the article!  I think the connection to pseudoscience needs to be clarified, or the article made more general and less creationism-specific and the category tag removed entirely.\n","------------------------------------------------------------\n","(and if such phrase exists, it would be provided by search engine even if mentioned page is not available as a whole)\n","------------------------------------------------------------\n",", 9 October 2007 (UTC)\n","\n","P.s, the delta function I use in the example above as  is the Kronecker delta, not the Dirac delta. The Kronecker delta is in , whereas the Dirac delta is not in L2[0,1].\n","\n","Incidentally, in response to the request for examples, L2(X) is not a RKHS for X any open subset of . On the other hand, Hardy spaces usually are RKHSs, using various variants of the Szego kernel.\n","\n","For example , the Hilbert space of L2 functions on the circle is not a RKHS. On the other hand, , the space of L2 functions on the circle which are tangential limits of holomorphic functions on the disc, is.  14:18\n","------------------------------------------------------------\n","]]- 10 September 1910, [[[Couvet]] )\n","------------------------------------------------------------\n","Negro league baseball task-force talk].\n","------------------------------------------------------------\n","List of My Three Sons episodes \n","\n","I saw you redirected two of the season episode lists back to the article a month ago, which removed them from both articles, since it was transcluded. I substituted the list back into the main article, and was about to remove the links back to the season lists, but the other seasons still have separate pages. I thought I'd just ask for your thoughts. It might be easier to just undo the substitution and redirects. -XT+\n","------------------------------------------------------------\n","Quoting Loremaster: In order to make sure you understand me\n","------------------------------------------------------------\n"]}],"source":["# Analyze some text columns\n","random_comments = train_data['comment_text'].sample(n=10, random_state=42)\n","for comment in random_comments.values:\n","    print(comment)\n","    print('------' * 10)"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.1.1 Tokenization\n","\n","* Tokenize the text data using `word_tokenize`\n","* Explain the importance of tokenization"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:50:03.345107Z","iopub.status.busy":"2024-04-30T20:50:03.344655Z","iopub.status.idle":"2024-04-30T20:50:03.354383Z","shell.execute_reply":"2024-04-30T20:50:03.353149Z","shell.execute_reply.started":"2024-04-30T20:50:03.345052Z"},"trusted":true},"outputs":[],"source":["def decontracted(phrase):\n","    # specific\n","    phrase = re.sub(r\"won't\", \"will not\", phrase)\n","    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n","\n","    # general\n","    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n","    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n","    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n","    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n","    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n","    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n","    return phrase"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.1.2 Stopword Removal\n","\n","* Remove stop words using `stopwords`\n","* Explain the importance of stopword removal"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:50:04.350037Z","iopub.status.busy":"2024-04-30T20:50:04.349064Z","iopub.status.idle":"2024-04-30T20:50:04.362405Z","shell.execute_reply":"2024-04-30T20:50:04.361031Z","shell.execute_reply.started":"2024-04-30T20:50:04.350001Z"},"trusted":true},"outputs":[],"source":["stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n","            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n","            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n","            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n","            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n","            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n","            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n","            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n","            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n","            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n","            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n","            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n","            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n","            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n","            'won', \"won't\", 'wouldn', \"wouldn't\"])"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.1.3 Lemmatization\n","\n","* Lemmatize words using `WordNetLemmatizer`\n","* Explain the importance of lemmatization"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:53:30.797103Z","iopub.status.busy":"2024-04-30T20:53:30.796631Z","iopub.status.idle":"2024-04-30T20:53:30.805459Z","shell.execute_reply":"2024-04-30T20:53:30.804063Z","shell.execute_reply.started":"2024-04-30T20:53:30.797068Z"},"trusted":true},"outputs":[],"source":["# NLTK\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess_1(sentence: str):\n","    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n","    sentence = decontracted(sentence)\n","    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n","    sentence = re.sub('[^A-Za-z]+',' ', sentence)\n","    sentence = word_tokenize(sentence)\n","    \n","    sentence = ' '.join(lemmatizer.lemmatize(word.lower()) for word in sentence if word.lower() not in stopwords)\n","    return sentence"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:53:52.744896Z","iopub.status.busy":"2024-04-30T20:53:52.744517Z","iopub.status.idle":"2024-04-30T20:53:52.751575Z","shell.execute_reply":"2024-04-30T20:53:52.750614Z","shell.execute_reply.started":"2024-04-30T20:53:52.744869Z"},"trusted":true},"outputs":[],"source":["# SPACY\n","def preprocess_2(sentence: str):\n","    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n","    sentence = decontracted(sentence)\n","    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n","    sentence = re.sub('[^A-Za-z]+',' ', sentence)\n","    doc = nlp(sentence)\n","    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop]\n","    return ' '.join(lemmatized_tokens)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T20:54:27.422893Z","iopub.status.busy":"2024-04-30T20:54:27.422492Z","iopub.status.idle":"2024-04-30T20:54:27.554046Z","shell.execute_reply":"2024-04-30T20:54:27.552793Z","shell.execute_reply.started":"2024-04-30T20:54:27.422862Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Explanation edit username Hardcore Metallica Fan revert vandalism closure gas vote New York Dolls FAC remove template talk page retire\n","------------------------------------------------------------\n","d aww match background colour seemingly stuck thank talk January UTC\n","------------------------------------------------------------\n","hey man try edit war guy constantly remove relevant information talk edit instead talk page care formatting actual info\n","------------------------------------------------------------\n","  real suggestion improvement wonder section statistic later subsection type accident think reference need tidying exact format ie date format etc later preference format style reference want let know appear backlog article review guess delay reviewer turn list relevant form eg Wikipedia good article nomination Transport\n","------------------------------------------------------------\n","sir hero chance remember page\n","------------------------------------------------------------\n"]}],"source":["# DEMO of comments after preprocessing \n","# * Tokenize\n","# * Stopwords Removal\n","# * Lemmatize\n","sample_data = train_data.head()\n","\n","result = sample_data['comment_text'].apply(lambda x: preprocess_2(x))\n","\n","for sentence in result:\n","    print(sentence)\n","    print('------' * 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Demo\n","train_data['processed_comments'] = train_data['comment_text'].apply(lambda x: preprocess(x))"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2 Feature Engineering"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.2.1 TF-IDF\n","\n","* Extract relevant features from the text data using TF-IDF\n","* Explain the importance of TF-IDF"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T21:16:30.665771Z","iopub.status.busy":"2024-04-30T21:16:30.665333Z","iopub.status.idle":"2024-04-30T21:16:30.679862Z","shell.execute_reply":"2024-04-30T21:16:30.678716Z","shell.execute_reply.started":"2024-04-30T21:16:30.665739Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["some sample features(unique words in the corpus) ['accident' 'actual' 'appear' 'article' 'aww' 'background' 'backlog'\n"," 'care' 'chance' 'closure' 'colour' 'constantly' 'date' 'delay' 'dolls'\n"," 'edit' 'eg' 'etc' 'exact' 'explanation' 'fac' 'fan' 'form' 'format'\n"," 'formatting' 'gas' 'good' 'guess' 'guy' 'hardcore' 'hero' 'hey' 'ie'\n"," 'improvement' 'info' 'information' 'instead' 'january' 'know' 'later'\n"," 'let' 'list' 'man' 'match' 'metallica' 'need' 'new' 'nomination' 'page'\n"," 'preference' 'real' 'reference' 'relevant' 'remember' 'remove' 'retire'\n"," 'revert' 'review' 'reviewer' 'section' 'seemingly' 'sir' 'statistic'\n"," 'stuck' 'style' 'subsection' 'suggestion' 'talk' 'template' 'thank'\n"," 'think' 'tidying' 'transport' 'try' 'turn' 'type' 'username' 'utc'\n"," 'vandalism' 'vote' 'want' 'war' 'wikipedia' 'wonder' 'york']\n","======================================================================================================================================================\n","the type of count vectorizer  <class 'scipy.sparse._csr.csr_matrix'>\n","the shape of out text TFIDF vectorizer  (5, 85)\n","the number of unique words including both unigrams and bigrams  85\n"]}],"source":["# Demo\n","tf_idf = TfidfVectorizer()\n","tf_idf.fit(result)\n","print(\"some sample features(unique words in the corpus)\",tf_idf.get_feature_names_out())\n","print('====='*30)\n","\n","final_tf_idf = tf_idf.transform(result)\n","print(\"the type of count vectorizer \",type(final_tf_idf))\n","print(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\n","print(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tf_idf = TfidfVectorizer()\n","tf_idf.fit(train_data['processed_comments'])\n","print(\"some sample features(unique words in the corpus)\",tf_idf.get_feature_names()[0:10])\n","print('='*50)\n","\n","final_tf_idf = tf_idf.transform(train_data['processed_comments'])\n","print(\"the type of count vectorizer \",type(final_tf_idf))\n","print(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\n","print(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.2.2 Word Embeddings\n","\n","* Extract relevant features from the text data using word embeddings\n","* Explain the importance of word embeddings"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Model Selection and Training\n","--------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["### 4.1 Model Selection"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.1.1 Algorithm Selection\n","\n","* Select a suitable machine learning algorithm for the task (e.g., Logistic Regression, SVM, Random Forest)\n","* Justify the choice of algorithm based on the problem and data characteristics"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.1.2 Hyperparameter Tuning\n","\n","* Explain the importance of hyperparameter tuning\n","* Discuss the methods used for hyperparameter tuning (e.g., grid search, random search)"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2 Model Training"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.2.1 Data Split\n","\n","* Split the data into training and testing sets using `train_test_split`\n","* Explain the importance of data splitting"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.2.2 Model Training\n","\n","* Train the selected model on the training data\n","* Explain the training process"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Model Evaluation\n","---------------------"]},{"cell_type":"markdown","metadata":{},"source":["### 5.1 Model Performance Metrics"]},{"cell_type":"markdown","metadata":{},"source":["#### 5.1.1 Accuracy\n","\n","* Evaluate the model using accuracy as a performance metric\n","* Explain the importance of accuracy"]},{"cell_type":"markdown","metadata":{},"source":["#### 5.1.2 F1-Score\n","\n","* Evaluate the model using F1-score as a performance metric\n","* Explain the importance of F1-score"]},{"cell_type":"markdown","metadata":{},"source":["#### 5.1.3 ROC-AUC\n","\n","* Evaluate the model using ROC-AUC as a performance metric\n","* Explain the importance of ROC-AUC"]},{"cell_type":"markdown","metadata":{},"source":["### 5.2 Model Comparison"]},{"cell_type":"markdown","metadata":{},"source":["#### 5.2.1 Model Comparison Metrics\n","\n","* Compare the performance of different models using relevant metrics\n","* Explain the importance of model comparison"]},{"cell_type":"markdown","metadata":{},"source":["#### 5.2.2 Model Selection\n","\n","* Select the best-performing model based on the comparison results\n","* Explain the justification for the chosen model"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Model Deployment\n","---------------------"]},{"cell_type":"markdown","metadata":{},"source":["### 6.1 Model Deployment Options"]},{"cell_type":"markdown","metadata":{},"source":["#### 6.1.1 API Deployment\n","\n","* Discuss the options for deploying the trained model as an API\n","* Explain the advantages and disadvantages of API deployment"]},{"cell_type":"markdown","metadata":{},"source":["#### 6.1.2 Web Application Deployment\n","\n","* Discuss the options for deploying the trained model as a web application\n","* Explain the advantages and disadvantages of web application deployment"]},{"cell_type":"markdown","metadata":{},"source":["### 6.2 Model Serving"]},{"cell_type":"markdown","metadata":{},"source":["#### 6.2.1 Model Serving Options\n","\n","* Discuss the options for serving the trained model (e.g., TensorFlow Serving, AWS SageMaker)\n","* Explain the advantages and disadvantages of each option"]},{"cell_type":"markdown","metadata":{},"source":["#### 6.2.2 Model Maintenance\n","\n","* Explain the importance of model maintenance\n","* Discuss the methods used for model maintenance (e.g., model updating, model retraining)"]},{"cell_type":"markdown","metadata":{},"source":["## 7. Conclusion\n","--------------"]},{"cell_type":"markdown","metadata":{},"source":["### 7.1 Summary\n","\n","* Summarize the key findings and results of the project\n","* Discuss the limitations and potential improvements of the project"]},{"cell_type":"markdown","metadata":{},"source":["### 7.2 Future Work\n","\n","* Provide recommendations for future work\n","* Discuss the potential applications of the project"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":44219,"sourceId":8076,"sourceType":"competition"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
