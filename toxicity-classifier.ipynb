{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-03T13:15:28.528517Z","iopub.execute_input":"2024-05-03T13:15:28.528920Z","iopub.status.idle":"2024-05-03T13:15:29.551747Z","shell.execute_reply.started":"2024-05-03T13:15:28.528888Z","shell.execute_reply":"2024-05-03T13:15:29.550718Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# /kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:01:13.682872Z","iopub.execute_input":"2024-04-30T20:01:13.683499Z","iopub.status.idle":"2024-04-30T20:01:13.688724Z","shell.execute_reply.started":"2024-04-30T20:01:13.683428Z","shell.execute_reply":"2024-04-30T20:01:13.687536Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# !git clone https://github.com/Muhammad-Shah/Toxity-Classifier-App.git\n# !git status\n# !git add --all\n!git config --global user.email \"\"\n!git config --global user.name \"\"\n# !git commit -m \"testing\"\n!git push","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-30T20:01:13.887001Z","iopub.execute_input":"2024-04-30T20:01:13.887434Z","iopub.status.idle":"2024-04-30T20:01:16.924141Z","shell.execute_reply.started":"2024-04-30T20:01:13.887401Z","shell.execute_reply":"2024-04-30T20:01:16.922293Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"fatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/Toxity-Classifier-App","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:01:16.927372Z","iopub.execute_input":"2024-04-30T20:01:16.928555Z","iopub.status.idle":"2024-04-30T20:01:16.938690Z","shell.execute_reply.started":"2024-04-30T20:01:16.928461Z","shell.execute_reply":"2024-04-30T20:01:16.937300Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[Errno 2] No such file or directory: '/kaggle/working/Toxity-Classifier-App'\n/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\nimport spacy\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nimport nltk\nfrom spacy import load\nnltk.download('omw-1.4')\nnltk.download('wordnet2022')\nnlp = load('en_core_web_sm')\n\n! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet # temp fix for lookup error.\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T13:15:35.859128Z","iopub.execute_input":"2024-05-03T13:15:35.860225Z","iopub.status.idle":"2024-05-03T13:15:45.803558Z","shell.execute_reply.started":"2024-05-03T13:15:35.860182Z","shell.execute_reply":"2024-05-03T13:15:45.802068Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data] Downloading package wordnet2022 to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet2022.zip.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Toxity Classifier NLP Project\n=====================================","metadata":{}},{"cell_type":"markdown","source":"## 1. Introduction\n---------------","metadata":{}},{"cell_type":"markdown","source":"### 1.1 Project Overview\n\n* This project aims to develop a toxicity classifier to identify toxic comments online.\n* The proliferation of toxic comments online has significant negative impacts on individuals and communities, necessitating the development of effective detection and mitigation strategies.","metadata":{}},{"cell_type":"markdown","source":"### 1.2 Problem Statement\n\n* Classify text as toxic or non-toxic to prevent online harassment and promote respectful communication.\n* To create a safer and more inclusive online environment, free from hate speech and cyberbullying.","metadata":{}},{"cell_type":"markdown","source":"## 2. Data Exploration\n---------------------","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Data Loading","metadata":{}},{"cell_type":"markdown","source":"#### 2.1.1 Load Dataset\n\n* Load the dataset into a Pandas dataframe\n* Display the first few rows of the data using `head()`","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(filepath_or_buffer='/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:49:54.131175Z","iopub.execute_input":"2024-04-30T20:49:54.131565Z","iopub.status.idle":"2024-04-30T20:49:56.068374Z","shell.execute_reply.started":"2024-04-30T20:49:54.131538Z","shell.execute_reply":"2024-04-30T20:49:56.067520Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### 2.1.2 Data Source\n\n* **Data Source:** Jigsaw Toxic Comment Classification Challenge on Kaggle\n* **Data Collection:** The dataset was collected from Wikipedia comments and annotated by human raters for toxicity.\n* **Data Description:** The dataset consists of a CSV file containing approximately 159,571 comments, each labeled as toxic or non-toxic.\n* **Data Download:** The dataset can be downloaded from the Kaggle competition page: [/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip](/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip)","metadata":{}},{"cell_type":"markdown","source":"### 2.1.3 Dataset Description\n\nYou are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n\n* toxic\n* severe_toxic\n* obscene\n* threat\n* insult\n* identity_hate\n\n`You must create a model which predicts a probability of each type of toxicity for each comment`","metadata":{}},{"cell_type":"markdown","source":"### 2.2 Data Summary","metadata":{}},{"cell_type":"markdown","source":"#### 2.2.1 Data Statistics\n\n* Provide a summary of the data using `info()` and `describe()`\n* Visualize the data distribution using plots (e.g., histograms, box plots)","metadata":{}},{"cell_type":"code","source":"train_data.info(verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:49:56.069793Z","iopub.execute_input":"2024-04-30T20:49:56.070120Z","iopub.status.idle":"2024-04-30T20:49:56.133424Z","shell.execute_reply.started":"2024-04-30T20:49:56.070095Z","shell.execute_reply":"2024-04-30T20:49:56.132493Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 159571 entries, 0 to 159570\nData columns (total 8 columns):\n #   Column         Non-Null Count   Dtype \n---  ------         --------------   ----- \n 0   id             159571 non-null  object\n 1   comment_text   159571 non-null  object\n 2   toxic          159571 non-null  int64 \n 3   severe_toxic   159571 non-null  int64 \n 4   obscene        159571 non-null  int64 \n 5   threat         159571 non-null  int64 \n 6   insult         159571 non-null  int64 \n 7   identity_hate  159571 non-null  int64 \ndtypes: int64(6), object(2)\nmemory usage: 9.7+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:49:56.134405Z","iopub.execute_input":"2024-04-30T20:49:56.134682Z","iopub.status.idle":"2024-04-30T20:49:56.180734Z","shell.execute_reply.started":"2024-04-30T20:49:56.134660Z","shell.execute_reply":"2024-04-30T20:49:56.179409Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"               toxic   severe_toxic        obscene         threat  \\\ncount  159571.000000  159571.000000  159571.000000  159571.000000   \nmean        0.095844       0.009996       0.052948       0.002996   \nstd         0.294379       0.099477       0.223931       0.054650   \nmin         0.000000       0.000000       0.000000       0.000000   \n25%         0.000000       0.000000       0.000000       0.000000   \n50%         0.000000       0.000000       0.000000       0.000000   \n75%         0.000000       0.000000       0.000000       0.000000   \nmax         1.000000       1.000000       1.000000       1.000000   \n\n              insult  identity_hate  \ncount  159571.000000  159571.000000  \nmean        0.049364       0.008805  \nstd         0.216627       0.093420  \nmin         0.000000       0.000000  \n25%         0.000000       0.000000  \n50%         0.000000       0.000000  \n75%         0.000000       0.000000  \nmax         1.000000       1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>159571.000000</td>\n      <td>159571.000000</td>\n      <td>159571.000000</td>\n      <td>159571.000000</td>\n      <td>159571.000000</td>\n      <td>159571.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.095844</td>\n      <td>0.009996</td>\n      <td>0.052948</td>\n      <td>0.002996</td>\n      <td>0.049364</td>\n      <td>0.008805</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.294379</td>\n      <td>0.099477</td>\n      <td>0.223931</td>\n      <td>0.054650</td>\n      <td>0.216627</td>\n      <td>0.093420</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### 2.2.2 Data Quality\n\n* Check for missing values and handle them accordingly\n* Check for duplicates and handle them accordingly","metadata":{}},{"cell_type":"code","source":"train_data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:49:56.183192Z","iopub.execute_input":"2024-04-30T20:49:56.183634Z","iopub.status.idle":"2024-04-30T20:49:56.234946Z","shell.execute_reply.started":"2024-04-30T20:49:56.183594Z","shell.execute_reply":"2024-04-30T20:49:56.233729Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"id               0\ncomment_text     0\ntoxic            0\nsevere_toxic     0\nobscene          0\nthreat           0\ninsult           0\nidentity_hate    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"columns = list(train_data.columns[1:])\ntrain_data.duplicated(subset=columns).value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:49:56.236891Z","iopub.execute_input":"2024-04-30T20:49:56.237332Z","iopub.status.idle":"2024-04-30T20:49:56.483587Z","shell.execute_reply.started":"2024-04-30T20:49:56.237294Z","shell.execute_reply":"2024-04-30T20:49:56.482457Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"False    159571\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"duplicates = train_data[train_data.duplicated(subset='comment_text', keep=False)]\nprint(f\"Number of duplicates: {len(duplicates)}\")\nprint(duplicates)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:49:56.485173Z","iopub.execute_input":"2024-04-30T20:49:56.485527Z","iopub.status.idle":"2024-04-30T20:49:56.579300Z","shell.execute_reply.started":"2024-04-30T20:49:56.485500Z","shell.execute_reply":"2024-04-30T20:49:56.578306Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Number of duplicates: 0\nEmpty DataFrame\nColumns: [id, comment_text, toxic, severe_toxic, obscene, threat, insult, identity_hate]\nIndex: []\n","output_type":"stream"}]},{"cell_type":"code","source":"for col in train_data.columns[2:]:\n    print(f\"Unique values in column {col}: {train_data[col].unique()}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:49:56.688773Z","iopub.execute_input":"2024-04-30T20:49:56.689114Z","iopub.status.idle":"2024-04-30T20:49:56.704110Z","shell.execute_reply.started":"2024-04-30T20:49:56.689088Z","shell.execute_reply":"2024-04-30T20:49:56.703272Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Unique values in column toxic: [0 1]\nUnique values in column severe_toxic: [0 1]\nUnique values in column obscene: [0 1]\nUnique values in column threat: [0 1]\nUnique values in column insult: [0 1]\nUnique values in column identity_hate: [0 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"for col in train_data.columns[2:]:\n    print(f\"The distribution of toxicity labels in column: {train_data[col].value_counts()}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:49:56.914561Z","iopub.execute_input":"2024-04-30T20:49:56.915267Z","iopub.status.idle":"2024-04-30T20:49:56.935288Z","shell.execute_reply.started":"2024-04-30T20:49:56.915224Z","shell.execute_reply":"2024-04-30T20:49:56.934199Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"The distribution of toxicity labels in column: toxic\n0    144277\n1     15294\nName: count, dtype: int64\nThe distribution of toxicity labels in column: severe_toxic\n0    157976\n1      1595\nName: count, dtype: int64\nThe distribution of toxicity labels in column: obscene\n0    151122\n1      8449\nName: count, dtype: int64\nThe distribution of toxicity labels in column: threat\n0    159093\n1       478\nName: count, dtype: int64\nThe distribution of toxicity labels in column: insult\n0    151694\n1      7877\nName: count, dtype: int64\nThe distribution of toxicity labels in column: identity_hate\n0    158166\n1      1405\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 2.3 Data Quality Check","metadata":{}},{"cell_type":"markdown","source":"#### 2.3.1 Handle Missing Values\n\n* Explain how missing values were handled\n* Provide justification for the chosen method","metadata":{}},{"cell_type":"code","source":"# NO Missing values are found","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:49:58.540538Z","iopub.execute_input":"2024-04-30T20:49:58.540900Z","iopub.status.idle":"2024-04-30T20:49:58.545296Z","shell.execute_reply.started":"2024-04-30T20:49:58.540874Z","shell.execute_reply":"2024-04-30T20:49:58.544294Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3.2 Handle Duplicates\n\n* Explain how duplicates were handled\n* Provide justification for the chosen method","metadata":{}},{"cell_type":"code","source":"# NO Duplicates are found","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:49:59.492426Z","iopub.execute_input":"2024-04-30T20:49:59.492830Z","iopub.status.idle":"2024-04-30T20:49:59.498586Z","shell.execute_reply.started":"2024-04-30T20:49:59.492800Z","shell.execute_reply":"2024-04-30T20:49:59.497310Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## 3. Data Preprocessing\n----------------------","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Text Preprocessing\n\nText preprocessing is the process of cleaning and transforming raw text data into a format that is suitable for analysis or modeling by removing unnecessary characters, converting all text to lowercase, removing stop words, and stemming or lemmatizing words to their base form.","metadata":{}},{"cell_type":"code","source":"# Analyze some text columns\nrandom_comments = train_data['comment_text'].sample(n=10, random_state=42)\nfor comment in random_comments.values:\n    print(comment)\n    print('------' * 10)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:50:01.364266Z","iopub.execute_input":"2024-04-30T20:50:01.364674Z","iopub.status.idle":"2024-04-30T20:50:01.377376Z","shell.execute_reply.started":"2024-04-30T20:50:01.364636Z","shell.execute_reply":"2024-04-30T20:50:01.376202Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Geez, are you forgetful!  We've already discussed why Marx  was  not an anarchist, i.e. he wanted to use a State to mold his 'socialist man.'  Ergo, he is a statist - the opposite of an  anarchist.  I know a guy who says that, when he gets old and his teeth fall out, he'll quit eating meat.  Would you call him a vegetarian?\n------------------------------------------------------------\nCarioca RFA \n\nThanks for your support on my request for adminship.\n\nThe final outcome was (31/4/1), so I am now an administrator. If you have any comments or concerns on my actions as an administrator, please let me know. Thank you!\n------------------------------------------------------------\n\"\n\n Birthday \n\nNo worries, It's what I do ;)Enjoy ur day|talk|e \"\n------------------------------------------------------------\nPseudoscience category? \n\nI'm assuming that this article is in the pseudoscience category because of its association with creationism.  However, there are modern, scientifically-accepted variants of catastrophism that have nothing to do with creationism — and they're even mentioned in the article!  I think the connection to pseudoscience needs to be clarified, or the article made more general and less creationism-specific and the category tag removed entirely.\n------------------------------------------------------------\n(and if such phrase exists, it would be provided by search engine even if mentioned page is not available as a whole)\n------------------------------------------------------------\n, 9 October 2007 (UTC)\n\nP.s, the delta function I use in the example above as  is the Kronecker delta, not the Dirac delta. The Kronecker delta is in , whereas the Dirac delta is not in L2[0,1].\n\nIncidentally, in response to the request for examples, L2(X) is not a RKHS for X any open subset of . On the other hand, Hardy spaces usually are RKHSs, using various variants of the Szego kernel.\n\nFor example , the Hilbert space of L2 functions on the circle is not a RKHS. On the other hand, , the space of L2 functions on the circle which are tangential limits of holomorphic functions on the disc, is.  14:18\n------------------------------------------------------------\n]]- 10 September 1910, [[[Couvet]] )\n------------------------------------------------------------\nNegro league baseball task-force talk].\n------------------------------------------------------------\nList of My Three Sons episodes \n\nI saw you redirected two of the season episode lists back to the article a month ago, which removed them from both articles, since it was transcluded. I substituted the list back into the main article, and was about to remove the links back to the season lists, but the other seasons still have separate pages. I thought I'd just ask for your thoughts. It might be easier to just undo the substitution and redirects. -XT+\n------------------------------------------------------------\nQuoting Loremaster: In order to make sure you understand me\n------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 3.1.1 Tokenization\n\n* Tokenize the text data using `word_tokenize`\n* Explain the importance of tokenization","metadata":{}},{"cell_type":"code","source":"def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:50:03.344655Z","iopub.execute_input":"2024-04-30T20:50:03.345107Z","iopub.status.idle":"2024-04-30T20:50:03.354383Z","shell.execute_reply.started":"2024-04-30T20:50:03.345052Z","shell.execute_reply":"2024-04-30T20:50:03.353149Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.2 Stopword Removal\n\n* Remove stop words using `stopwords`\n* Explain the importance of stopword removal","metadata":{}},{"cell_type":"code","source":"stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:50:04.349064Z","iopub.execute_input":"2024-04-30T20:50:04.350037Z","iopub.status.idle":"2024-04-30T20:50:04.362405Z","shell.execute_reply.started":"2024-04-30T20:50:04.350001Z","shell.execute_reply":"2024-04-30T20:50:04.361031Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.3 Lemmatization\n\n* Lemmatize words using `WordNetLemmatizer`\n* Explain the importance of lemmatization","metadata":{}},{"cell_type":"code","source":"# NLTK\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_1(sentence: str):\n    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n    sentence = decontracted(sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+',' ', sentence)\n    sentence = word_tokenize(sentence)\n    \n    sentence = ' '.join(lemmatizer.lemmatize(word.lower()) for word in sentence if word.lower() not in stopwords)\n    return sentence","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:53:30.796631Z","iopub.execute_input":"2024-04-30T20:53:30.797103Z","iopub.status.idle":"2024-04-30T20:53:30.805459Z","shell.execute_reply.started":"2024-04-30T20:53:30.797068Z","shell.execute_reply":"2024-04-30T20:53:30.804063Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# SPACY\ndef preprocess_2(sentence: str):\n    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n    sentence = decontracted(sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+',' ', sentence)\n    doc = nlp(sentence)\n    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop]\n    return ' '.join(lemmatized_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:53:52.744517Z","iopub.execute_input":"2024-04-30T20:53:52.744896Z","iopub.status.idle":"2024-04-30T20:53:52.751575Z","shell.execute_reply.started":"2024-04-30T20:53:52.744869Z","shell.execute_reply":"2024-04-30T20:53:52.750614Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# DEMO of comments after preprocessing \n# * Tokenize\n# * Stopwords Removal\n# * Lemmatize\nsample_data = train_data.head()\n\nresult = sample_data['comment_text'].apply(lambda x: preprocess_2(x))\n\nfor sentence in result:\n    print(sentence)\n    print('------' * 10)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:54:27.422492Z","iopub.execute_input":"2024-04-30T20:54:27.422893Z","iopub.status.idle":"2024-04-30T20:54:27.554046Z","shell.execute_reply.started":"2024-04-30T20:54:27.422862Z","shell.execute_reply":"2024-04-30T20:54:27.552793Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Explanation edit username Hardcore Metallica Fan revert vandalism closure gas vote New York Dolls FAC remove template talk page retire\n------------------------------------------------------------\nd aww match background colour seemingly stuck thank talk January UTC\n------------------------------------------------------------\nhey man try edit war guy constantly remove relevant information talk edit instead talk page care formatting actual info\n------------------------------------------------------------\n  real suggestion improvement wonder section statistic later subsection type accident think reference need tidying exact format ie date format etc later preference format style reference want let know appear backlog article review guess delay reviewer turn list relevant form eg Wikipedia good article nomination Transport\n------------------------------------------------------------\nsir hero chance remember page\n------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"# Demo\ntrain_data['processed_comments'] = train_data['comment_text'].apply(lambda x: preprocess(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"#### 3.2.1 TF-IDF\n\n* Extract relevant features from the text data using TF-IDF\n* Explain the importance of TF-IDF","metadata":{}},{"cell_type":"code","source":"# Demo\ntf_idf = TfidfVectorizer()\ntf_idf.fit(result)\nprint(\"some sample features(unique words in the corpus)\",tf_idf.get_feature_names_out())\nprint('====='*30)\n\nfinal_tf_idf = tf_idf.transform(result)\nprint(\"the type of count vectorizer \",type(final_tf_idf))\nprint(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])","metadata":{"execution":{"iopub.status.busy":"2024-04-30T21:16:30.665333Z","iopub.execute_input":"2024-04-30T21:16:30.665771Z","iopub.status.idle":"2024-04-30T21:16:30.679862Z","shell.execute_reply.started":"2024-04-30T21:16:30.665739Z","shell.execute_reply":"2024-04-30T21:16:30.678716Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"some sample features(unique words in the corpus) ['accident' 'actual' 'appear' 'article' 'aww' 'background' 'backlog'\n 'care' 'chance' 'closure' 'colour' 'constantly' 'date' 'delay' 'dolls'\n 'edit' 'eg' 'etc' 'exact' 'explanation' 'fac' 'fan' 'form' 'format'\n 'formatting' 'gas' 'good' 'guess' 'guy' 'hardcore' 'hero' 'hey' 'ie'\n 'improvement' 'info' 'information' 'instead' 'january' 'know' 'later'\n 'let' 'list' 'man' 'match' 'metallica' 'need' 'new' 'nomination' 'page'\n 'preference' 'real' 'reference' 'relevant' 'remember' 'remove' 'retire'\n 'revert' 'review' 'reviewer' 'section' 'seemingly' 'sir' 'statistic'\n 'stuck' 'style' 'subsection' 'suggestion' 'talk' 'template' 'thank'\n 'think' 'tidying' 'transport' 'try' 'turn' 'type' 'username' 'utc'\n 'vandalism' 'vote' 'want' 'war' 'wikipedia' 'wonder' 'york']\n======================================================================================================================================================\nthe type of count vectorizer  <class 'scipy.sparse._csr.csr_matrix'>\nthe shape of out text TFIDF vectorizer  (5, 85)\nthe number of unique words including both unigrams and bigrams  85\n","output_type":"stream"}]},{"cell_type":"code","source":"tf_idf = TfidfVectorizer()\ntf_idf.fit(train_data['processed_comments'])\nprint(\"some sample features(unique words in the corpus)\",tf_idf.get_feature_names()[0:10])\nprint('='*50)\n\nfinal_tf_idf = tf_idf.transform(train_data['processed_comments'])\nprint(\"the type of count vectorizer \",type(final_tf_idf))\nprint(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2.2 Word Embeddings\n\n* Extract relevant features from the text data using word embeddings\n* Explain the importance of word embeddings","metadata":{}},{"cell_type":"markdown","source":"## 4. Model Selection and Training\n--------------------------------","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Model Selection","metadata":{}},{"cell_type":"markdown","source":"#### 4.1.1 Algorithm Selection\n\n* Select a suitable machine learning algorithm for the task (e.g., Logistic Regression, SVM, Random Forest)\n* Justify the choice of algorithm based on the problem and data characteristics","metadata":{}},{"cell_type":"markdown","source":"#### 4.1.2 Hyperparameter Tuning\n\n* Explain the importance of hyperparameter tuning\n* Discuss the methods used for hyperparameter tuning (e.g., grid search, random search)","metadata":{}},{"cell_type":"markdown","source":"### 4.2 Model Training","metadata":{}},{"cell_type":"markdown","source":"#### 4.2.1 Data Split\n\n* Split the data into training and testing sets using `train_test_split`\n* Explain the importance of data splitting","metadata":{}},{"cell_type":"markdown","source":"#### 4.2.2 Model Training\n\n* Train the selected model on the training data\n* Explain the training process","metadata":{}},{"cell_type":"markdown","source":"## 5. Model Evaluation\n---------------------","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Model Performance Metrics","metadata":{}},{"cell_type":"markdown","source":"#### 5.1.1 Accuracy\n\n* Evaluate the model using accuracy as a performance metric\n* Explain the importance of accuracy","metadata":{}},{"cell_type":"markdown","source":"#### 5.1.2 F1-Score\n\n* Evaluate the model using F1-score as a performance metric\n* Explain the importance of F1-score","metadata":{}},{"cell_type":"markdown","source":"#### 5.1.3 ROC-AUC\n\n* Evaluate the model using ROC-AUC as a performance metric\n* Explain the importance of ROC-AUC","metadata":{}},{"cell_type":"markdown","source":"### 5.2 Model Comparison","metadata":{}},{"cell_type":"markdown","source":"#### 5.2.1 Model Comparison Metrics\n\n* Compare the performance of different models using relevant metrics\n* Explain the importance of model comparison","metadata":{}},{"cell_type":"markdown","source":"#### 5.2.2 Model Selection\n\n* Select the best-performing model based on the comparison results\n* Explain the justification for the chosen model","metadata":{}},{"cell_type":"markdown","source":"## 6. Model Deployment\n---------------------","metadata":{}},{"cell_type":"markdown","source":"### 6.1 Model Deployment Options","metadata":{}},{"cell_type":"markdown","source":"#### 6.1.1 API Deployment\n\n* Discuss the options for deploying the trained model as an API\n* Explain the advantages and disadvantages of API deployment","metadata":{}},{"cell_type":"markdown","source":"#### 6.1.2 Web Application Deployment\n\n* Discuss the options for deploying the trained model as a web application\n* Explain the advantages and disadvantages of web application deployment","metadata":{}},{"cell_type":"markdown","source":"### 6.2 Model Serving","metadata":{}},{"cell_type":"markdown","source":"#### 6.2.1 Model Serving Options\n\n* Discuss the options for serving the trained model (e.g., TensorFlow Serving, AWS SageMaker)\n* Explain the advantages and disadvantages of each option","metadata":{}},{"cell_type":"markdown","source":"#### 6.2.2 Model Maintenance\n\n* Explain the importance of model maintenance\n* Discuss the methods used for model maintenance (e.g., model updating, model retraining)","metadata":{}},{"cell_type":"markdown","source":"## 7. Conclusion\n--------------","metadata":{}},{"cell_type":"markdown","source":"### 7.1 Summary\n\n* Summarize the key findings and results of the project\n* Discuss the limitations and potential improvements of the project","metadata":{}},{"cell_type":"markdown","source":"### 7.2 Future Work\n\n* Provide recommendations for future work\n* Discuss the potential applications of the project","metadata":{}}]}